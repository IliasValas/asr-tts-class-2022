{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ASR-TTS-class 2021 We will be using this for additional explanations and instructions regarding the code and methodologies we are discussing during our meetings. Keep in mind that the code can be found here: [[https://github.com/nassosoassos/asr-tts-class-2021]] Happy making!","title":"Home"},{"location":"#welcome-to-asr-tts-class-2021","text":"We will be using this for additional explanations and instructions regarding the code and methodologies we are discussing during our meetings. Keep in mind that the code can be found here: [[https://github.com/nassosoassos/asr-tts-class-2021]] Happy making!","title":"Welcome to ASR-TTS-class 2021"},{"location":"prepare/","text":"Preparing the data for Kaldi The theory Considering that our dataset is a set of utterances, each with a unique id, we will need to create three files (using the names of the files as these are created for the mini-librispeech recipe): A wav.scp file providing the mapping of the ids to the corresponding audio files. There is a bit more to that if the files are not in .wav format but we will get there soon. wav.scp id000001 /data/audio/file1.wav id000002 /data/audio/file2.wav ... A text file linking each id to the text of the utterance. text id000001 This is the text uttered in the first file id000002 I am now reading the second line ... A utt2spk file mapping each utterance id to a speaker id. If speakers are unknown we can use a separate speaker id per utterance. id000001 spk_id_0001 id000002 spk_id_0001 ... There is a spk2utt file that is also needed but we will create this using a utility script already provided by Kaldi. In practice Reading the tsv files (or creating such files if they do not exist) We will use pandas for that: import pandas import csv part_file='train.tsv' # The .tsv file has the following columns: # client_id, path, sentence, ... dataset = pandas.read_csv(part_file, delimiter='\\t') dataset['id'] = dataset['client_id']+ '_' + dataset['path'].str.replace('.mp3','', regex=True) # Doing this here since Kaldi tools seem to be expecting it later in the pipeline dataset = dataset.sort_values(by=['id']) But what happens when there are no .tsv files? If we just have a directory of audio recordings and the transcriptions already in a separate file tagged by the correspoding audio filename: transcriptions.txt wav_filename_1.wav This is one utterance. wav_filename_2.wav This is a second utterance. ... We can then create a basic .tsv file as follows (again using python): import glob import os.path audio_data_dir = '/data/audio' transcriptions_file = '/data/transcriptions.txt' tsv_file = 'all.tsv' # The audio files are in separate folders per speaker (named after speaker id) wav_files = glob.glob(f\"{audio_data_dir}/*/.wav\") # No transcription file, no honey with open(transcriptions_file, 'r') as trans: transcriptions = {} speakers = {} paths = {} for line in trans: cline = line.rstrip() line_info = cline.split(' ') utt_id, ext = os.path.splitext(line_info[0]) if len(line_info)>1: transcriptions[utt_id] = \" \".join(line_info[1:]) for recording in wav_files: path_head, wav_basename = os.path.split(recording) path_head, spk_id = os.path.split() utt_id, ext = os.path.splitext(wav_basename) speakers[utt_id] = spk_id paths[utt_id] = wav_basename with open(tsv_file, 'w') as tsv: tsv_file.write(\"\\t\".join((\"client_id\", \"path\", \"sentence\"))+\"\\n\") for utt_id in transcriptions: sentence = transcriptions[utt_id] speaker = speakers[utt_id] audio = paths[utt_id] tsv_file.write(f\"{speaker}\\t{audio}\\t{sentence}\\n\") And what if the transcriptions are inside json files along with other metadata? That's indeed true for a dataset I am aware of. To be discussed later. Creating the wav.scp file The russian commonvoice dataset only has .mp3 (and not wav) audio files. So, we will need to preprocess the data to get the wav files Kaldi requires. A tool to do that is sox using the following command lines (in bash): for f in /data/audio/*/*.mp3; do sox $f -r 16000 $f.wav; done By the way, we have specified, using the -r switch, that we want to also downsample the data to 16kHz. And then we can proceed with creating the 'wav.scp' by just doing the following in python (appending to the script we started in the previous section): dataset['abs_path'] = os.path.audio_data_dir + '/' + dataset['path'] + '.wav' dataset[['id', 'abs_path']].to_csv(wav_scp_file, sep=' ', index=False, header=False) Kaldi also provides a neat way to do this conversion from mp3 to wav right before the feature extraction step (that we will discuss later on) avoiding in this way to store the wav files and saving space. It should be noted here that wav files can be significantly bigger than their compressed (mp3) versions (depending on the exact sampling rates and quality the wav file could be ~10 times bigger or even more). This is done by providing a command line that would write the converted audio directly to stdout so that it could be piped into a following command (bash magic). The command line would look like this: sox file.mp3 -t wav -r 16000 - | By providing the - at the input of sox instead of a filename we are guiding the tool to generate the output and flush it to the standard output. And then we can pipe this into the following command. So, to write the corresponding wav.scp file in python we would do the following: # Create wav.scp file dataset['mp3_to_wav'] = 'sox ' + audio_data_dir + '/' + dataset['path'] + ' -r 16000 -t wav - |' dataset[['id', 'mp3_to_wav']].to_csv(wav_scp_file, quotechar=' ', sep=' ', index=False, header=False) So, what about the quotechar ? As one would easily notice if omitting to use this, the generated file would look as follows: wav.scp id000001 \"sox f1.mp3 -r 16000 -t wav - |\" ... where the command has been written within quotes. That's the right thing to do given that our separator is the space character that is also included in the command line we want to register. Without the quotes our file won't be readable as a space-separated file. That's not what we are aiming at, however. Our goal is to just separate the initial column, i.e., the id, from the rest of the line with a space (that's what Kaldi needs, it complains otherwise) and we are just tweaking a bit the .to_csv method of pandas dataframes to make it work as we want. So, instead of a \"quote\" we instruct pandas to use a space character. Creating the text file Having done most of the work already, the only thing we need to take care when creating the text file is to clean punctuation (well, and also use python3 to make sure encodings are handled properly out-of-the-box). The code in python follows: # Create trans_file dataset['sentence'] = dataset['sentence'].str.replace(r'[^\\w\\s]+',' ', regex=True) dataset[['id', 'sentence']].to_csv(trans_file, quotechar=' ', sep=' ', index=False, header=False) Using the quotechar here for the reasons explained above. And here comes the utt2spk file The easiest of all. Mind the quotechar game again: # Create utt2spk file dataset[['id', 'client_id']].to_csv(utt2spk_file, quotechar=' ', sep=' ', index=False, header=False) And that's our \"process_tsv_file.py\" script. Could be named: \"data_prep_from_tsv.py\". That would probably be more appropriate. Final steps There are two more things to be done: 1. Create the spk2utt file. 2. Validate that these data files are all following Kaldi conventions. There is a script for that. We will use the utils/utt2spk_to_spk2utt.pl for that, copying it from the \"minilibrispeech\". Actually, let's copy the entire \"utils\", \"steps\", and \"local\" folders into our working directory since we will be using various tools and scripts from there: cp -rp mini_librispeech/s5/{utils,steps,local} cv_russian/s5 And we will properly modify the \"local/data_prep.sh\" script to create our own: #!/usr/bin/env bash # Copyright 2020 Nassos Katsamanis # Apache 2.0 if [ \"$#\" -ne 3 ]; then echo \"Usage: $0 <src-dir> <part-info> <dst-dir>\" echo \"e.g.: $0 /data/russian train.tsv data/train\" exit 1 fi src=$1 part=$2 dst=$3 mkdir -p $dst || exit 1; [ ! -d $src ] && echo \"$0: no such directory $src\" && exit 1; # If the following files exist, then delete them, we will recreate them. wav_scp=$dst/wav.scp; [[ -f \"$wav_scp\" ]] && rm $wav_scp trans=$dst/text; [[ -f \"$trans\" ]] && rm $trans utt2spk=$dst/utt2spk; [[ -f \"$utt2spk\" ]] && rm $utt2spk # Process tsv files to extract all required information PYTHONIOENCODING=UTF-8 python3 local/process_tsv_file.py $part $src/clips $wav_scp $trans $utt2spk spk2utt=$dst/spk2utt utils/utt2spk_to_spk2utt.pl <$utt2spk >$spk2utt || exit 1 And here comes the data validation part: # Data validation # To measure the number of lines in the corresponding files ntrans=$(wc -l <$trans) nutt2spk=$(wc -l <$utt2spk) ! [ \"$ntrans\" -eq \"$nutt2spk\" ] && \\ echo \"Inconsistent #transcripts($ntrans) and #utt2spk($nutt2spk)\" && exit 1; utils/validate_data_dir.sh --no-feats $dst || exit 1; echo \"$0: successfully prepared data in $dst\" exit 0 Data validation must be 100% successful before proceeding with the rest of the recipe. Back to the recipe And this is how our recipe looks now. Notice how we need to run data preparation for each of our \"training\", \"development\", and \"test\" datasets. #!/usr/bin/env bash src_data=/data/russian/cv-corpus-6.1-2020-12-11/ru/ # These are for configuration and to add tools to the path so that they are # accessible from the command line. . ./cmd.sh . ./path.sh stage=0 . utils/parse_options.sh set -euo pipefail mkdir -p data # Data preparation # No need to split data into train, dev, and test. Commonvoice data is already split. if [ $stage -le 1 ]; then # format the data as Kaldi data directories for part in train dev test; do # For each of these parts we need to have a separate subfolder with the following files in it: # wav.scp text utt2spk spk2utt local/russian_data_prep.sh $src_data $src_data/$part.tsv data/$part done fi","title":"Prepare the data for Kaldi"},{"location":"prepare/#preparing-the-data-for-kaldi","text":"","title":"Preparing the data for Kaldi"},{"location":"prepare/#the-theory","text":"Considering that our dataset is a set of utterances, each with a unique id, we will need to create three files (using the names of the files as these are created for the mini-librispeech recipe): A wav.scp file providing the mapping of the ids to the corresponding audio files. There is a bit more to that if the files are not in .wav format but we will get there soon. wav.scp id000001 /data/audio/file1.wav id000002 /data/audio/file2.wav ... A text file linking each id to the text of the utterance. text id000001 This is the text uttered in the first file id000002 I am now reading the second line ... A utt2spk file mapping each utterance id to a speaker id. If speakers are unknown we can use a separate speaker id per utterance. id000001 spk_id_0001 id000002 spk_id_0001 ... There is a spk2utt file that is also needed but we will create this using a utility script already provided by Kaldi.","title":"The theory"},{"location":"prepare/#in-practice","text":"","title":"In practice"},{"location":"prepare/#reading-the-tsv-files-or-creating-such-files-if-they-do-not-exist","text":"We will use pandas for that: import pandas import csv part_file='train.tsv' # The .tsv file has the following columns: # client_id, path, sentence, ... dataset = pandas.read_csv(part_file, delimiter='\\t') dataset['id'] = dataset['client_id']+ '_' + dataset['path'].str.replace('.mp3','', regex=True) # Doing this here since Kaldi tools seem to be expecting it later in the pipeline dataset = dataset.sort_values(by=['id']) But what happens when there are no .tsv files? If we just have a directory of audio recordings and the transcriptions already in a separate file tagged by the correspoding audio filename: transcriptions.txt wav_filename_1.wav This is one utterance. wav_filename_2.wav This is a second utterance. ... We can then create a basic .tsv file as follows (again using python): import glob import os.path audio_data_dir = '/data/audio' transcriptions_file = '/data/transcriptions.txt' tsv_file = 'all.tsv' # The audio files are in separate folders per speaker (named after speaker id) wav_files = glob.glob(f\"{audio_data_dir}/*/.wav\") # No transcription file, no honey with open(transcriptions_file, 'r') as trans: transcriptions = {} speakers = {} paths = {} for line in trans: cline = line.rstrip() line_info = cline.split(' ') utt_id, ext = os.path.splitext(line_info[0]) if len(line_info)>1: transcriptions[utt_id] = \" \".join(line_info[1:]) for recording in wav_files: path_head, wav_basename = os.path.split(recording) path_head, spk_id = os.path.split() utt_id, ext = os.path.splitext(wav_basename) speakers[utt_id] = spk_id paths[utt_id] = wav_basename with open(tsv_file, 'w') as tsv: tsv_file.write(\"\\t\".join((\"client_id\", \"path\", \"sentence\"))+\"\\n\") for utt_id in transcriptions: sentence = transcriptions[utt_id] speaker = speakers[utt_id] audio = paths[utt_id] tsv_file.write(f\"{speaker}\\t{audio}\\t{sentence}\\n\") And what if the transcriptions are inside json files along with other metadata? That's indeed true for a dataset I am aware of. To be discussed later.","title":"Reading the tsv files (or creating such files if they do not exist)"},{"location":"prepare/#creating-the-wavscp-file","text":"The russian commonvoice dataset only has .mp3 (and not wav) audio files. So, we will need to preprocess the data to get the wav files Kaldi requires. A tool to do that is sox using the following command lines (in bash): for f in /data/audio/*/*.mp3; do sox $f -r 16000 $f.wav; done By the way, we have specified, using the -r switch, that we want to also downsample the data to 16kHz. And then we can proceed with creating the 'wav.scp' by just doing the following in python (appending to the script we started in the previous section): dataset['abs_path'] = os.path.audio_data_dir + '/' + dataset['path'] + '.wav' dataset[['id', 'abs_path']].to_csv(wav_scp_file, sep=' ', index=False, header=False) Kaldi also provides a neat way to do this conversion from mp3 to wav right before the feature extraction step (that we will discuss later on) avoiding in this way to store the wav files and saving space. It should be noted here that wav files can be significantly bigger than their compressed (mp3) versions (depending on the exact sampling rates and quality the wav file could be ~10 times bigger or even more). This is done by providing a command line that would write the converted audio directly to stdout so that it could be piped into a following command (bash magic). The command line would look like this: sox file.mp3 -t wav -r 16000 - | By providing the - at the input of sox instead of a filename we are guiding the tool to generate the output and flush it to the standard output. And then we can pipe this into the following command. So, to write the corresponding wav.scp file in python we would do the following: # Create wav.scp file dataset['mp3_to_wav'] = 'sox ' + audio_data_dir + '/' + dataset['path'] + ' -r 16000 -t wav - |' dataset[['id', 'mp3_to_wav']].to_csv(wav_scp_file, quotechar=' ', sep=' ', index=False, header=False) So, what about the quotechar ? As one would easily notice if omitting to use this, the generated file would look as follows: wav.scp id000001 \"sox f1.mp3 -r 16000 -t wav - |\" ... where the command has been written within quotes. That's the right thing to do given that our separator is the space character that is also included in the command line we want to register. Without the quotes our file won't be readable as a space-separated file. That's not what we are aiming at, however. Our goal is to just separate the initial column, i.e., the id, from the rest of the line with a space (that's what Kaldi needs, it complains otherwise) and we are just tweaking a bit the .to_csv method of pandas dataframes to make it work as we want. So, instead of a \"quote\" we instruct pandas to use a space character.","title":"Creating the wav.scp file"},{"location":"prepare/#creating-the-text-file","text":"Having done most of the work already, the only thing we need to take care when creating the text file is to clean punctuation (well, and also use python3 to make sure encodings are handled properly out-of-the-box). The code in python follows: # Create trans_file dataset['sentence'] = dataset['sentence'].str.replace(r'[^\\w\\s]+',' ', regex=True) dataset[['id', 'sentence']].to_csv(trans_file, quotechar=' ', sep=' ', index=False, header=False) Using the quotechar here for the reasons explained above.","title":"Creating the text file"},{"location":"prepare/#and-here-comes-the-utt2spk-file","text":"The easiest of all. Mind the quotechar game again: # Create utt2spk file dataset[['id', 'client_id']].to_csv(utt2spk_file, quotechar=' ', sep=' ', index=False, header=False) And that's our \"process_tsv_file.py\" script. Could be named: \"data_prep_from_tsv.py\". That would probably be more appropriate.","title":"And here comes the utt2spk file"},{"location":"prepare/#final-steps","text":"There are two more things to be done: 1. Create the spk2utt file. 2. Validate that these data files are all following Kaldi conventions. There is a script for that. We will use the utils/utt2spk_to_spk2utt.pl for that, copying it from the \"minilibrispeech\". Actually, let's copy the entire \"utils\", \"steps\", and \"local\" folders into our working directory since we will be using various tools and scripts from there: cp -rp mini_librispeech/s5/{utils,steps,local} cv_russian/s5 And we will properly modify the \"local/data_prep.sh\" script to create our own: #!/usr/bin/env bash # Copyright 2020 Nassos Katsamanis # Apache 2.0 if [ \"$#\" -ne 3 ]; then echo \"Usage: $0 <src-dir> <part-info> <dst-dir>\" echo \"e.g.: $0 /data/russian train.tsv data/train\" exit 1 fi src=$1 part=$2 dst=$3 mkdir -p $dst || exit 1; [ ! -d $src ] && echo \"$0: no such directory $src\" && exit 1; # If the following files exist, then delete them, we will recreate them. wav_scp=$dst/wav.scp; [[ -f \"$wav_scp\" ]] && rm $wav_scp trans=$dst/text; [[ -f \"$trans\" ]] && rm $trans utt2spk=$dst/utt2spk; [[ -f \"$utt2spk\" ]] && rm $utt2spk # Process tsv files to extract all required information PYTHONIOENCODING=UTF-8 python3 local/process_tsv_file.py $part $src/clips $wav_scp $trans $utt2spk spk2utt=$dst/spk2utt utils/utt2spk_to_spk2utt.pl <$utt2spk >$spk2utt || exit 1 And here comes the data validation part: # Data validation # To measure the number of lines in the corresponding files ntrans=$(wc -l <$trans) nutt2spk=$(wc -l <$utt2spk) ! [ \"$ntrans\" -eq \"$nutt2spk\" ] && \\ echo \"Inconsistent #transcripts($ntrans) and #utt2spk($nutt2spk)\" && exit 1; utils/validate_data_dir.sh --no-feats $dst || exit 1; echo \"$0: successfully prepared data in $dst\" exit 0 Data validation must be 100% successful before proceeding with the rest of the recipe.","title":"Final steps"},{"location":"prepare/#back-to-the-recipe","text":"And this is how our recipe looks now. Notice how we need to run data preparation for each of our \"training\", \"development\", and \"test\" datasets. #!/usr/bin/env bash src_data=/data/russian/cv-corpus-6.1-2020-12-11/ru/ # These are for configuration and to add tools to the path so that they are # accessible from the command line. . ./cmd.sh . ./path.sh stage=0 . utils/parse_options.sh set -euo pipefail mkdir -p data # Data preparation # No need to split data into train, dev, and test. Commonvoice data is already split. if [ $stage -le 1 ]; then # format the data as Kaldi data directories for part in train dev test; do # For each of these parts we need to have a separate subfolder with the following files in it: # wav.scp text utt2spk spk2utt local/russian_data_prep.sh $src_data $src_data/$part.tsv data/$part done fi","title":"Back to the recipe"},{"location":"recipe/","text":"How to initialize a new Kaldi recipe So, what is really a recipe in this context? It's the set of instructions the machine should follow to actually generate, or train, if you like, a speech recognition engine for a specific task. One such task could be speech recognition for Mandarin Chinese or spoken command recognition for smart home applications, or you name it. By \"engine\" here we essentially refer to a \"final.mdl\" file including all acoustic modeling information, a \"HCLG.fst\" with all language information and a few other configuration files. These files can be loaded into a so-called decoder (one of the \"tools\" Kaldi provides) to actually transform speech into text. There are already multiple recipes to build such engines included with Kaldi and we will be re-using them as much as possible. That is probably the most important take-home-message of this exercise: there is already a lot of work put into Kaldi, we just need to understand a bit better what kind of \"lego pieces\", or, even better, \"lego structures\" we have available and then go ahead to build our \"castle\". First pick a task ( or, more typically, the task will pick you ) In our case we just want to build a speech recognition engine for dictation in Russian and we will use that as a working case. Other cases one can work on: We will be using commonvoice data as provided by Mozilla. I used to be a fan of voxforge in the past for open source data but it seems that Mozilla is actually doing a very good job on that these days: offers many languages and really lots of crowdsourced data. Use mini-librispeech as our guide We will then simply run the mini-librispeech recipe to make sure that it works smoothly and that we get back the results we expect. I can do that by just running: cd asr\\_recipes/mini\\_librispeech/s5 ./run.sh We would typically just need to replace 'queue.pl' inside cmd.sh file with 'run.pl' for the recipe to run end-to-end without problems: sed -i 's/queue/run/g' cmd.sh The recipe will reach stage 9 and will typically exit with a message saying that there are no GPUs available, etc. That is fine for now. You can check that the recipe has worked by running the one-liner in the header of the RESULTS file inside the s5 folder. This command (or multiple commands, better) will look for the WERs (Word Error Rates) estimated for various different configurations and different snapshots of the speech recognition engine, pick the lowest of them and display it: for x in exp/*/decode\\*; do [ -d $x ] && [[ $x =~ \"$1\" ]] && \\ grep WER $x/wer_\\* | \\ utils/best_wer.sh; done The output should be something like the following, presenting the numbers for insertions/deletions/substitutions for three tests using different language models (small, medium, large): WER 13.45 [ 2708 / 20138, 358 ins, 330 del, 2020 sub ] exp/tri3b/decode_nosp_tglarge_dev_clean_2/wer_17_0.0 WER 16.25 [ 3273 / 20138, 332 ins, 485 del, 2456 sub ] exp/tri3b/decode_nosp_tgmed_dev_clean_2/wer_16_0.0 WER 18.10 [ 3645 / 20138, 332 ins, 603 del, 2710 sub ] exp/tri3b/decode_nosp_tgsmall_dev_clean_2/wer_16_0.0 Create the folder for the new recipe Let's name it cv-russian. And also create a subfolder 's5' into it: cd asr\\_recipes mkdir -p cv-russian/s5 This 's5' is legacy from one of the initial recipes developed, namely one for the Wall Street Journal task (still available as wsj inside asr_recipes). There is used to be multiple alternatives, from s1 to s5. The best of them, 's5', was the one essentially inherited and properly modified in other recipes that were developed later on. And let's create a new run.sh file in ths folder. Initial recipe structure The recipe will need to include four (or five, optionally) main parts: The preamble, where we will initialize stuff, create the necessary folder structure and add tools to our working path If we don't yet have the dataset available, we will need to also include a \"download data\" section, where all audio files, transcriptions, and, if available, the language model as well are downloaded from the web (or transferred from elsewhere) The main data preparation part, where speech data are preprocessed so that they can be fed to Kaldi tools for model training and testing The acoustic model training part, where we will be feeding the data to the appropriate Kaldi tools to build the engine. Our data Before moving any deeper into the recipe development, let's first review our data. We will be looking for audio recordings (.wav, .mp3 files or other formats) and the corresponding transcriptions (.txt, .json, .tsv files or similar). These are the basic ingredients: we need the audio (in relatively small chunks, typically, e.g., 10-15 seconds of duration max.) and what was actually said in the audio in the form of text. For the tutorial, we will be using Russian data from commonvoice as made available [[https://commonvoice.mozilla.org/en/datasets|here]]. It's impressive how they've gathered data from so many languages in there. Other sources for publicly available speech data include: [[http://www.voxforge.org|voxforge]], [[https://www.openslr.org/12|librispeech]], and [[https://www.openslr.org/51/|TED Talks]]. Here is a nice collection, by the way, of relevant publicly available resources [[https://www.openslr.org/index.html]]. Assuming for now that data have already been downloaded and are stored in a locally mounted folder as unzipped from the downloaded dataset (ru.tar.gz. Use: tar xvfz ru.tar.gz to unzip): data=/other/data/russian/cv-corpus-6.1-2020-12-11/ru/ This is what we have in there: clips dev.tsv invalidated.tsv other.tsv reported.tsv testing test.tsv text train.tsv validated.tsv Our audio is in .mp3 format inside the clips folder. We will some extra preprocessing for these as we will see later on. common_voice_ru_18849003.mp3 common_voice_ru_18849004.mp3 common_voice_ru_18849005.mp3 common_voice_ru_18849006.mp3 ... And then we have these .tsv files (watch for tab separated values), with metadata: train.tsv client_id path sentence up_votes down_votes age gender accent ... 87e2683ece common_voice_ru_18931630.mp3 \u0412\u0430\u0436\u043d\u043e, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0447\u0435\u0442\u043a\u043e ... It's where we will get our transcriptions from, there is a sentence field for each path with the text in Russian. There's also speaker information, i.e., client_id , that will come in handy during training. So, here's the plan: 1. We have a list of the files we will be using for training, testing, and development (coming from the corresponding .tsv files) along with the corresponding transcriptions. It won't be hard to actually convert these to the appropriate format for kaldi. We will see how that happens in the following. 2. We also have the audio recordings as .mp3 files that we will need to convert to wav for further processing. Tools like ffmpeg or sox can be used to the rescue.","title":"Create a new ASR recipe"},{"location":"recipe/#how-to-initialize-a-new-kaldi-recipe","text":"So, what is really a recipe in this context? It's the set of instructions the machine should follow to actually generate, or train, if you like, a speech recognition engine for a specific task. One such task could be speech recognition for Mandarin Chinese or spoken command recognition for smart home applications, or you name it. By \"engine\" here we essentially refer to a \"final.mdl\" file including all acoustic modeling information, a \"HCLG.fst\" with all language information and a few other configuration files. These files can be loaded into a so-called decoder (one of the \"tools\" Kaldi provides) to actually transform speech into text. There are already multiple recipes to build such engines included with Kaldi and we will be re-using them as much as possible. That is probably the most important take-home-message of this exercise: there is already a lot of work put into Kaldi, we just need to understand a bit better what kind of \"lego pieces\", or, even better, \"lego structures\" we have available and then go ahead to build our \"castle\".","title":"How to initialize a new Kaldi recipe"},{"location":"recipe/#first-pick-a-task-or-more-typically-the-task-will-pick-you","text":"In our case we just want to build a speech recognition engine for dictation in Russian and we will use that as a working case. Other cases one can work on: We will be using commonvoice data as provided by Mozilla. I used to be a fan of voxforge in the past for open source data but it seems that Mozilla is actually doing a very good job on that these days: offers many languages and really lots of crowdsourced data.","title":"First pick a task ( or, more typically, the task will pick you )"},{"location":"recipe/#use-mini-librispeech-as-our-guide","text":"We will then simply run the mini-librispeech recipe to make sure that it works smoothly and that we get back the results we expect. I can do that by just running: cd asr\\_recipes/mini\\_librispeech/s5 ./run.sh We would typically just need to replace 'queue.pl' inside cmd.sh file with 'run.pl' for the recipe to run end-to-end without problems: sed -i 's/queue/run/g' cmd.sh The recipe will reach stage 9 and will typically exit with a message saying that there are no GPUs available, etc. That is fine for now. You can check that the recipe has worked by running the one-liner in the header of the RESULTS file inside the s5 folder. This command (or multiple commands, better) will look for the WERs (Word Error Rates) estimated for various different configurations and different snapshots of the speech recognition engine, pick the lowest of them and display it: for x in exp/*/decode\\*; do [ -d $x ] && [[ $x =~ \"$1\" ]] && \\ grep WER $x/wer_\\* | \\ utils/best_wer.sh; done The output should be something like the following, presenting the numbers for insertions/deletions/substitutions for three tests using different language models (small, medium, large): WER 13.45 [ 2708 / 20138, 358 ins, 330 del, 2020 sub ] exp/tri3b/decode_nosp_tglarge_dev_clean_2/wer_17_0.0 WER 16.25 [ 3273 / 20138, 332 ins, 485 del, 2456 sub ] exp/tri3b/decode_nosp_tgmed_dev_clean_2/wer_16_0.0 WER 18.10 [ 3645 / 20138, 332 ins, 603 del, 2710 sub ] exp/tri3b/decode_nosp_tgsmall_dev_clean_2/wer_16_0.0","title":"Use mini-librispeech as our guide"},{"location":"recipe/#create-the-folder-for-the-new-recipe","text":"Let's name it cv-russian. And also create a subfolder 's5' into it: cd asr\\_recipes mkdir -p cv-russian/s5 This 's5' is legacy from one of the initial recipes developed, namely one for the Wall Street Journal task (still available as wsj inside asr_recipes). There is used to be multiple alternatives, from s1 to s5. The best of them, 's5', was the one essentially inherited and properly modified in other recipes that were developed later on. And let's create a new run.sh file in ths folder.","title":"Create the folder for the new recipe"},{"location":"recipe/#initial-recipe-structure","text":"The recipe will need to include four (or five, optionally) main parts: The preamble, where we will initialize stuff, create the necessary folder structure and add tools to our working path If we don't yet have the dataset available, we will need to also include a \"download data\" section, where all audio files, transcriptions, and, if available, the language model as well are downloaded from the web (or transferred from elsewhere) The main data preparation part, where speech data are preprocessed so that they can be fed to Kaldi tools for model training and testing The acoustic model training part, where we will be feeding the data to the appropriate Kaldi tools to build the engine.","title":"Initial recipe structure"},{"location":"recipe/#our-data","text":"Before moving any deeper into the recipe development, let's first review our data. We will be looking for audio recordings (.wav, .mp3 files or other formats) and the corresponding transcriptions (.txt, .json, .tsv files or similar). These are the basic ingredients: we need the audio (in relatively small chunks, typically, e.g., 10-15 seconds of duration max.) and what was actually said in the audio in the form of text. For the tutorial, we will be using Russian data from commonvoice as made available [[https://commonvoice.mozilla.org/en/datasets|here]]. It's impressive how they've gathered data from so many languages in there. Other sources for publicly available speech data include: [[http://www.voxforge.org|voxforge]], [[https://www.openslr.org/12|librispeech]], and [[https://www.openslr.org/51/|TED Talks]]. Here is a nice collection, by the way, of relevant publicly available resources [[https://www.openslr.org/index.html]]. Assuming for now that data have already been downloaded and are stored in a locally mounted folder as unzipped from the downloaded dataset (ru.tar.gz. Use: tar xvfz ru.tar.gz to unzip): data=/other/data/russian/cv-corpus-6.1-2020-12-11/ru/ This is what we have in there: clips dev.tsv invalidated.tsv other.tsv reported.tsv testing test.tsv text train.tsv validated.tsv Our audio is in .mp3 format inside the clips folder. We will some extra preprocessing for these as we will see later on. common_voice_ru_18849003.mp3 common_voice_ru_18849004.mp3 common_voice_ru_18849005.mp3 common_voice_ru_18849006.mp3 ... And then we have these .tsv files (watch for tab separated values), with metadata: train.tsv client_id path sentence up_votes down_votes age gender accent ... 87e2683ece common_voice_ru_18931630.mp3 \u0412\u0430\u0436\u043d\u043e, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0447\u0435\u0442\u043a\u043e ... It's where we will get our transcriptions from, there is a sentence field for each path with the text in Russian. There's also speaker information, i.e., client_id , that will come in handy during training. So, here's the plan: 1. We have a list of the files we will be using for training, testing, and development (coming from the corresponding .tsv files) along with the corresponding transcriptions. It won't be hard to actually convert these to the appropriate format for kaldi. We will see how that happens in the following. 2. We also have the audio recordings as .mp3 files that we will need to convert to wav for further processing. Tools like ffmpeg or sox can be used to the rescue.","title":"Our data"}]}